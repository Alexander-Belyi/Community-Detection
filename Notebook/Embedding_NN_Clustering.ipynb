{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L73c8k8DtR2w",
        "outputId": "a885df42-5c19-4c17-cb08-e687a95a0cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycombo\n",
            "  Downloading pycombo-0.1.7.tar.gz (136 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 136 kB 4.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11<3.0.0,>=2.6.1\n",
            "  Downloading pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 33.9 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<2.0,>=1.0\n",
            "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->pycombo) (3.8.0)\n",
            "Building wheels for collected packages: pycombo\n",
            "  Building wheel for pycombo (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycombo: filename=pycombo-0.1.7-cp37-cp37m-manylinux_2_27_x86_64.whl size=98830 sha256=9e19beca46226f3c9742a00b868b377976ed462204d0dc244d4b1c47ed62f386\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/52/18/4c1b80cd45c091e2c1ea442729343ac984dc66b3a678e2c251\n",
            "Successfully built pycombo\n",
            "Installing collected packages: pybind11, importlib-metadata, pycombo\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed importlib-metadata-1.7.0 pybind11-2.9.2 pycombo-0.1.7\n",
            "Collecting fastnode2vec\n",
            "  Downloading fastnode2vec-0.0.6-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (7.1.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (0.51.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->fastnode2vec) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->fastnode2vec) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->fastnode2vec) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->fastnode2vec) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->fastnode2vec) (0.34.0)\n",
            "Installing collected packages: fastnode2vec\n",
            "Successfully installed fastnode2vec-0.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pycombo\n",
        "!pip install fastnode2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aH23YvKuTW9y"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import pycombo\n",
        "import fastnode2vec\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_printoptions(sci_mode=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J0t3cana7mxf",
        "outputId": "0fb7ec05-b1b4-47c4-dca8-db406fe4cdea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nx.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S7bPMdVUTkPm"
      },
      "outputs": [],
      "source": [
        "def modularity_matrix(adj):\n",
        "    w_in = adj.sum(dim=0, keepdim=True)\n",
        "    w_out = adj.sum(dim=1, keepdim=True)\n",
        "    T = w_out.sum()\n",
        "    Q = adj / T - w_out * w_in / T ** 2\n",
        "    return Q\n",
        "\n",
        "def modularity(Q, partition):\n",
        "    return (Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum()\n",
        "\n",
        "def residential_page_rank_embedding(A, alpha=0.85):\n",
        "    A = A / A.sum(axis = 1)\n",
        "    n = A.shape[0]\n",
        "    AI = np.linalg.inv(np.eye(n) - A * alpha)\n",
        "    X = (1 - alpha) * AI\n",
        "    return X.transpose()\n",
        "\n",
        "def node2vec_embedding(G: nx.Graph, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42):\n",
        "    if nx.is_weighted(G):\n",
        "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1]), edge[2]['weight']) for edge in G.edges(data=True)],\n",
        "                directed=False, weighted=True)\n",
        "    else:\n",
        "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1])) for edge in G.edges(data=True)],\n",
        "                    directed=False, weighted=False)\n",
        "    n2v = fastnode2vec.Node2Vec(n2v_graph, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
        "    n2v.train(epochs=100)\n",
        "    n2v_embeddings = np.array([n2v.wv[str(node)] for node in G])\n",
        "    return n2v_embeddings\n",
        "\n",
        "def rpr_clustering(A: np.array, n_clusters=4, kmeans_runs=100, alpha=0.85, seed=42):\n",
        "    rpr_embedding = residential_page_rank_embedding(A, alpha)\n",
        "    rpr_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(rpr_embedding).labels_\n",
        "    return rpr_cluster_labels\n",
        "\n",
        "def n2v_clustering(G: nx.Graph, n_clusters=4, kmeans_runs=100, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42):\n",
        "    n2v_embeddings = node2vec_embedding(G, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
        "    n2v_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(n2v_embeddings).labels_\n",
        "    return n2v_cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPDPOtUuTnqH",
        "outputId": "2f0d1fb3-cb87-4585-d64e-7d73ed6b4e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading graph: 100%|██████████| 78/78 [00:00<00:00, 236213.51it/s]\n",
            "Training: 100%|██████████| 3400/3400 [00:02<00:00, 1603.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combo: 0.41978961209730403 , resid. page rank k-means: 0.4197896122932434 , node2vec kmeans: 0.39340895414352417\n",
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading graph: 100%|██████████| 254/254 [00:00<00:00, 479456.89it/s]\n",
            "Training: 100%|██████████| 7700/7700 [00:02<00:00, 3259.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combo: 0.566687983343249 , resid. page rank k-means: 0.5447605848312378 , node2vec kmeans: 0.5652743577957153\n",
            "------------------------------------\n",
            "SEED =  1\n",
            "Processing graph... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading graph: 100%|██████████| 78/78 [00:00<00:00, 234688.46it/s]\n",
            "Training: 100%|██████████| 3400/3400 [00:00<00:00, 3502.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combo: 0.41978961209730403 , resid. page rank k-means: 0.4197896122932434 , node2vec kmeans: 0.4197896122932434\n",
            "------------------------------------\n",
            "SEED =  1\n",
            "Processing graph... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading graph: 100%|██████████| 254/254 [00:00<00:00, 139920.31it/s]\n",
            "Training: 100%|██████████| 7700/7700 [00:02<00:00, 2633.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combo: 0.566687983343249 , resid. page rank k-means: 0.5447605848312378 , node2vec kmeans: 0.5422798991203308\n",
            "------------------------------------\n",
            "SEED =  2\n",
            "Processing graph... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading graph: 100%|██████████| 78/78 [00:00<00:00, 112001.27it/s]\n",
            "Training: 100%|██████████| 3400/3400 [00:00<00:00, 4284.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combo: 0.41978961209730403 , resid. page rank k-means: 0.4197896122932434 , node2vec kmeans: 0.4197896122932434\n",
            "------------------------------------\n",
            "SEED =  2\n",
            "Processing graph... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading graph: 100%|██████████| 254/254 [00:00<00:00, 193033.74it/s]\n",
            "Training: 100%|██████████| 7700/7700 [00:01<00:00, 4644.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combo: 0.566687983343249 , resid. page rank k-means: 0.5447605848312378 , node2vec kmeans: 0.5382205247879028\n"
          ]
        }
      ],
      "source": [
        "Gs =  [nx.from_numpy_array(nx.to_numpy_array(nx.karate_club_graph(), weight=None)), nx.les_miserables_graph()]\n",
        "for SEED in range(3):\n",
        "    for G in Gs:\n",
        "        print(\"------------------------------------\")\n",
        "        print(\"SEED = \", SEED)\n",
        "        print(\"Processing graph...\", G.name)\n",
        "        combo_comms, combo_mod = pycombo.execute(G)\n",
        "        n_comms = np.unique(list(combo_comms.values())).size\n",
        "        A = nx.to_numpy_array(G)\n",
        "        adj = torch.FloatTensor(A)\n",
        "        Q = modularity_matrix(adj)\n",
        "        rpr_C = rpr_clustering(A, n_clusters=n_comms, seed=SEED)\n",
        "        n2v_C = n2v_clustering(G, n_clusters=n_comms, seed=SEED)\n",
        "        print('combo:', combo_mod, ', resid. page rank k-means:', modularity(Q, rpr_C).item(), ', node2vec kmeans:', modularity(Q, n2v_C).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fwmyXMYvTcHf"
      },
      "outputs": [],
      "source": [
        "class GNNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(GNNLayer, self).__init__()\n",
        "        self.weight1 = nn.Parameter(torch.randn(in_features, out_features)) # 0.5 * torch.eye(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.randn(in_features, out_features)) # -0.5 * torch.ones(1, out_features))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, input):\n",
        "        v1 = torch.mm(input, self.weight1)\n",
        "        output = v1 + self.bias\n",
        "        output = F.dropout(output, p=self.dropout, training=self.training)\n",
        "        return output\n",
        "\n",
        "class GNN_MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(GNN_MLP, self).__init__()\n",
        "        self.n_layers = 1\n",
        "        self.hidden_dim = 8\n",
        "        if self.n_layers > 1:\n",
        "            layers = [GNNLayer(in_features, self.hidden_dim, dropout)]\n",
        "        else:\n",
        "            layers = [GNNLayer(in_features, out_features, dropout)]\n",
        "        for _ in range(self.n_layers-2):\n",
        "            layers.append(GNNLayer(self.hidden_dim, self.hidden_dim, dropout))\n",
        "        if self.n_layers > 1:\n",
        "            layers.append(GNNLayer(self.hidden_dim, out_features, dropout))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.n_layers - 1):\n",
        "            x = self.layers[i](x)\n",
        "            x = nn.ReLU(x)\n",
        "        x = self.layers[-1](x)\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "        #x = 1.0 + x - x.max(dim=-1, keepdim=True).values\n",
        "        #x = torch.clamp(x, 0, 1)\n",
        "        #x = x / x.sum(dim=-1, keepdim=True) #normalize st sum = 1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ibPTM41DTQ1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a235bb3-ddef-4259-f3b5-ed9ff7b433c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: -0.00834351 time: 0.0015s\n",
            "Epoch: 0301 Modularity: 0.11897559 time: 0.0009s\n",
            "Epoch: 0601 Modularity: 0.29895350 time: 0.0010s\n",
            "Epoch: 0901 Modularity: 0.36741373 time: 0.0011s\n",
            "Epoch: 1201 Modularity: 0.38858986 time: 0.0009s\n",
            "Epoch: 1501 Modularity: 0.39584810 time: 0.0011s\n",
            "Epoch: 1801 Modularity: 0.39941776 time: 0.0006s\n",
            "Epoch: 2101 Modularity: 0.40150204 time: 0.0006s\n",
            "Epoch: 2401 Modularity: 0.40284574 time: 0.0006s\n",
            "Epoch: 2701 Modularity: 0.40376967 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.40443480 time: 0.0006s\n",
            "Epoch: 3301 Modularity: 0.40492997 time: 0.0008s\n",
            "Epoch: 3601 Modularity: 0.40530834 time: 0.0015s\n",
            "Epoch: 3901 Modularity: 0.40560350 time: 0.0010s\n",
            "Epoch: 4201 Modularity: 0.40583745 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.40602544 time: 0.0006s\n",
            "Epoch: 4801 Modularity: 0.40617812 time: 0.0006s\n",
            "Epoch: 5101 Modularity: 0.40630323 time: 0.0006s\n",
            "Epoch: 5401 Modularity: 0.40640661 time: 0.0006s\n",
            "Epoch: 5701 Modularity: 0.40649247 time: 0.0006s\n",
            "Epoch: 6000 Modularity: 0.40656403 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 5.2929s\n",
            "tensor(-0.4066, grad_fn=<NegBackward0>)\n",
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: -0.00772001 time: 0.0025s\n",
            "Epoch: 0301 Modularity: 0.18118833 time: 0.0006s\n",
            "Epoch: 0601 Modularity: 0.43844554 time: 0.0006s\n",
            "Epoch: 0901 Modularity: 0.49764374 time: 0.0006s\n",
            "Epoch: 1201 Modularity: 0.51708418 time: 0.0006s\n",
            "Epoch: 1501 Modularity: 0.52492607 time: 0.0006s\n",
            "Epoch: 1801 Modularity: 0.52995539 time: 0.0006s\n",
            "Epoch: 2101 Modularity: 0.53434187 time: 0.0006s\n",
            "Epoch: 2401 Modularity: 0.54285622 time: 0.0011s\n",
            "Epoch: 2701 Modularity: 0.55154276 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.55740023 time: 0.0006s\n",
            "Epoch: 3301 Modularity: 0.55871755 time: 0.0006s\n",
            "Epoch: 3601 Modularity: 0.55938315 time: 0.0006s\n",
            "Epoch: 3901 Modularity: 0.55982846 time: 0.0007s\n",
            "Epoch: 4201 Modularity: 0.56015438 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.56040424 time: 0.0007s\n",
            "Epoch: 4801 Modularity: 0.56060112 time: 0.0007s\n",
            "Epoch: 5101 Modularity: 0.56075954 time: 0.0007s\n",
            "Epoch: 5401 Modularity: 0.56088859 time: 0.0006s\n",
            "Epoch: 5701 Modularity: 0.56099486 time: 0.0007s\n",
            "Epoch: 6000 Modularity: 0.56108284 time: 0.0007s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.0722s\n",
            "tensor(-0.5611, grad_fn=<NegBackward0>)\n",
            "0.5610828399658203\n",
            "------------------------------------\n",
            "SEED =  1\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: -0.01098188 time: 0.0013s\n",
            "Epoch: 0301 Modularity: 0.15605707 time: 0.0006s\n",
            "Epoch: 0601 Modularity: 0.31515008 time: 0.0006s\n",
            "Epoch: 0901 Modularity: 0.34915277 time: 0.0010s\n",
            "Epoch: 1201 Modularity: 0.35931855 time: 0.0006s\n",
            "Epoch: 1501 Modularity: 0.36369678 time: 0.0006s\n",
            "Epoch: 1801 Modularity: 0.36606535 time: 0.0006s\n",
            "Epoch: 2101 Modularity: 0.36752301 time: 0.0006s\n",
            "Epoch: 2401 Modularity: 0.36850545 time: 0.0006s\n",
            "Epoch: 2701 Modularity: 0.36922511 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.36978033 time: 0.0006s\n",
            "Epoch: 3301 Modularity: 0.37019485 time: 0.0006s\n",
            "Epoch: 3601 Modularity: 0.37050259 time: 0.0006s\n",
            "Epoch: 3901 Modularity: 0.37073782 time: 0.0010s\n",
            "Epoch: 4201 Modularity: 0.37092197 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.37106878 time: 0.0006s\n",
            "Epoch: 4801 Modularity: 0.37118730 time: 0.0006s\n",
            "Epoch: 5101 Modularity: 0.37128407 time: 0.0006s\n",
            "Epoch: 5401 Modularity: 0.37136379 time: 0.0006s\n",
            "Epoch: 5701 Modularity: 0.37142989 time: 0.0006s\n",
            "Epoch: 6000 Modularity: 0.37148494 time: 0.0007s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.8681s\n",
            "tensor(-0.3715, grad_fn=<NegBackward0>)\n",
            "------------------------------------\n",
            "SEED =  1\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: -0.00457799 time: 0.0013s\n",
            "Epoch: 0301 Modularity: 0.15859941 time: 0.0007s\n",
            "Epoch: 0601 Modularity: 0.41702899 time: 0.0007s\n",
            "Epoch: 0901 Modularity: 0.47181091 time: 0.0010s\n",
            "Epoch: 1201 Modularity: 0.48719975 time: 0.0006s\n",
            "Epoch: 1501 Modularity: 0.50452751 time: 0.0006s\n",
            "Epoch: 1801 Modularity: 0.51935714 time: 0.0006s\n",
            "Epoch: 2101 Modularity: 0.52309859 time: 0.0006s\n",
            "Epoch: 2401 Modularity: 0.52506137 time: 0.0006s\n",
            "Epoch: 2701 Modularity: 0.52653283 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.52753890 time: 0.0006s\n",
            "Epoch: 3301 Modularity: 0.52969682 time: 0.0006s\n",
            "Epoch: 3601 Modularity: 0.53367990 time: 0.0009s\n",
            "Epoch: 3901 Modularity: 0.53428149 time: 0.0006s\n",
            "Epoch: 4201 Modularity: 0.53461218 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.53484416 time: 0.0009s\n",
            "Epoch: 4801 Modularity: 0.53502011 time: 0.0006s\n",
            "Epoch: 5101 Modularity: 0.53515875 time: 0.0006s\n",
            "Epoch: 5401 Modularity: 0.53527051 time: 0.0006s\n",
            "Epoch: 5701 Modularity: 0.53536201 time: 0.0006s\n",
            "Epoch: 6000 Modularity: 0.53543723 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.9914s\n",
            "tensor(-0.5354, grad_fn=<NegBackward0>)\n",
            "0.5354372262954712\n",
            "------------------------------------\n",
            "SEED =  2\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: 0.00173515 time: 0.0011s\n",
            "Epoch: 0301 Modularity: 0.12520155 time: 0.0006s\n",
            "Epoch: 0601 Modularity: 0.25445750 time: 0.0006s\n",
            "Epoch: 0901 Modularity: 0.31566480 time: 0.0006s\n",
            "Epoch: 1201 Modularity: 0.33333430 time: 0.0006s\n",
            "Epoch: 1501 Modularity: 0.34145176 time: 0.0006s\n",
            "Epoch: 1801 Modularity: 0.35028589 time: 0.0008s\n",
            "Epoch: 2101 Modularity: 0.35431555 time: 0.0006s\n",
            "Epoch: 2401 Modularity: 0.36122227 time: 0.0006s\n",
            "Epoch: 2701 Modularity: 0.37522432 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.37748078 time: 0.0007s\n",
            "Epoch: 3301 Modularity: 0.37835607 time: 0.0006s\n",
            "Epoch: 3601 Modularity: 0.37885794 time: 0.0006s\n",
            "Epoch: 3901 Modularity: 0.37919128 time: 0.0006s\n",
            "Epoch: 4201 Modularity: 0.37943113 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.37961328 time: 0.0006s\n",
            "Epoch: 4801 Modularity: 0.37976095 time: 0.0006s\n",
            "Epoch: 5101 Modularity: 0.37993917 time: 0.0006s\n",
            "Epoch: 5401 Modularity: 0.38046926 time: 0.0006s\n",
            "Epoch: 5701 Modularity: 0.38063952 time: 0.0006s\n",
            "Epoch: 6000 Modularity: 0.38072234 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.8666s\n",
            "tensor(-0.3807, grad_fn=<NegBackward0>)\n",
            "------------------------------------\n",
            "SEED =  2\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: -0.00270017 time: 0.0012s\n",
            "Epoch: 0301 Modularity: 0.16449922 time: 0.0007s\n",
            "Epoch: 0601 Modularity: 0.44442728 time: 0.0006s\n",
            "Epoch: 0901 Modularity: 0.51144558 time: 0.0006s\n",
            "Epoch: 1201 Modularity: 0.52746302 time: 0.0007s\n",
            "Epoch: 1501 Modularity: 0.53438056 time: 0.0006s\n",
            "Epoch: 1801 Modularity: 0.53807628 time: 0.0007s\n",
            "Epoch: 2101 Modularity: 0.54033118 time: 0.0006s\n",
            "Epoch: 2401 Modularity: 0.54181057 time: 0.0006s\n",
            "Epoch: 2701 Modularity: 0.54283458 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.54357404 time: 0.0006s\n",
            "Epoch: 3301 Modularity: 0.54412580 time: 0.0007s\n",
            "Epoch: 3601 Modularity: 0.54454803 time: 0.0006s\n",
            "Epoch: 3901 Modularity: 0.54487765 time: 0.0006s\n",
            "Epoch: 4201 Modularity: 0.54513919 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.54534948 time: 0.0006s\n",
            "Epoch: 4801 Modularity: 0.54552025 time: 0.0010s\n",
            "Epoch: 5101 Modularity: 0.54566032 time: 0.0011s\n",
            "Epoch: 5401 Modularity: 0.54577595 time: 0.0006s\n",
            "Epoch: 5701 Modularity: 0.54587209 time: 0.0006s\n",
            "Epoch: 6000 Modularity: 0.54595214 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 4.0052s\n",
            "tensor(-0.5460, grad_fn=<NegBackward0>)\n",
            "0.5459521412849426\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(3):\n",
        "    for G in Gs:\n",
        "        print(\"------------------------------------\")\n",
        "        print(\"SEED = \", SEED)\n",
        "        print(\"Processing graph...\", G.name)\n",
        "        combo_comms, combo_mod = pycombo.execute(G)\n",
        "        n_comms = np.unique(list(combo_comms.values())).size\n",
        "        A = nx.to_numpy_array(G)\n",
        "        adj = torch.FloatTensor(A)\n",
        "        Q = modularity_matrix(adj)\n",
        "        \n",
        "        rpr_embedding = residential_page_rank_embedding(A, 0.85)\n",
        "        features = torch.FloatTensor(rpr_embedding)\n",
        "        best_best_mod = -1\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        t_total = time.time()\n",
        "        model = GNN_MLP(features.shape[1], n_comms + 2)\n",
        "        lr = 0.002\n",
        "        n_epochs = 6000\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            t_1run = time.time()\n",
        "            optimizer.zero_grad()\n",
        "            out_embed = model(features)\n",
        "            C = out_embed#[:, :n_comm]\n",
        "            Q1 = torch.mm(C.T, Q)\n",
        "            Q2 = torch.mm(Q1, C)\n",
        "            loss = torch.trace(Q2)\n",
        "            loss = -loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if epoch == 0 or loss < best_loss:\n",
        "                best_loss = loss #- torch.trace(Q)\n",
        "                best_C = C.data\n",
        "                best_embed = out_embed.data\n",
        "                best_epoch = epoch\n",
        "            if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
        "                #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                print('Epoch: {:04d}'.format(epoch + 1),\n",
        "                        'Modularity: {:.8f}'.format(-best_loss.item()),\n",
        "                        'time: {:.4f}s'.format(time.time() - t_1run))\n",
        "        print(\"Optimization Finished!\")\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "        print(best_loss)\n",
        "        best_best_mod = max(best_best_mod, -best_loss.item())\n",
        "        #print(best_embed)\n",
        "    print(best_best_mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Whnxlk_EZ0Iy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Embedding_NN_Clustering.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aa8c9e988bba47ec3e791b22c0bbb49eb66a938b6cde8fffd564472f09fd563a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}