{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding_NN_Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pycombo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L73c8k8DtR2w",
        "outputId": "be6943dd-c222-4e75-df99-7ff2a29c0c2d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycombo\n",
            "  Downloading pycombo-0.1.7.tar.gz (136 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 136 kB 5.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11<3.0.0,>=2.6.1\n",
            "  Downloading pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<2.0,>=1.0\n",
            "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->pycombo) (3.8.0)\n",
            "Building wheels for collected packages: pycombo\n",
            "  Building wheel for pycombo (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycombo: filename=pycombo-0.1.7-cp37-cp37m-manylinux_2_27_x86_64.whl size=98830 sha256=10d38a8559f52bcafe7fcc2f5de89460a6913875e28a730b41afb4f4e60ded8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/52/18/4c1b80cd45c091e2c1ea442729343ac984dc66b3a678e2c251\n",
            "Successfully built pycombo\n",
            "Installing collected packages: pybind11, importlib-metadata, pycombo\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed importlib-metadata-1.7.0 pybind11-2.9.2 pycombo-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import pycombo\n",
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_printoptions(sci_mode=False)"
      ],
      "metadata": {
        "id": "aH23YvKuTW9y"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def propEmbed(A, alpha = 0.95):\n",
        "    A = A / A.sum(axis = 1)\n",
        "    n = A.shape[0]\n",
        "    AI = np.linalg.inv(np.eye(n) - A * alpha)\n",
        "    X = (1 - alpha) * AI\n",
        "    return X\n",
        "\n",
        "\n",
        "def modularity_matrix(adj):\n",
        "    w_in = adj.sum(dim=0, keepdim=True)\n",
        "    w_out = adj.sum(dim=1, keepdim=True)\n",
        "    T = w_out.sum()\n",
        "    Q = adj / T - w_out * w_in / T ** 2\n",
        "    return Q\n",
        "\n",
        "def modularity(Q, partition):\n",
        "    return (Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum()"
      ],
      "metadata": {
        "id": "S7bPMdVUTkPm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#G =  nx.karate_club_graph()\n",
        "G = nx.les_miserables_graph()\n",
        "#print(G.edges(data=True))\n",
        "combo_comms, combo_mod = pycombo.execute(G)\n",
        "A = nx.to_numpy_array(G)\n",
        "X = propEmbed(A, alpha = 0.85)\n",
        "c = KMeans(n_clusters = 4, n_init=100).fit(X.transpose()).labels_\n",
        "adj = torch.FloatTensor(A)\n",
        "Q = modularity_matrix(adj)\n",
        "print(modularity(Q, c).item(), combo_mod)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPDPOtUuTnqH",
        "outputId": "ec89dc15-3821-4a21-9ea7-c6f7e14a3129"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.522426426410675 0.566687983343249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(GNNLayer, self).__init__()\n",
        "        self.weight1 = nn.Parameter(torch.randn(in_features, out_features)) # 0.5 * torch.eye(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.randn(in_features, out_features)) # -0.5 * torch.ones(1, out_features))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, input):\n",
        "        v1 = torch.mm(input, self.weight1)\n",
        "        output = v1 + self.bias\n",
        "        output = F.dropout(output, p=self.dropout, training=self.training)\n",
        "        return output\n",
        "\n",
        "class GNN_MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(GNN_MLP, self).__init__()\n",
        "        self.n_layers = 1\n",
        "        self.hidden_dim = 8\n",
        "        if self.n_layers > 1:\n",
        "            layers = [GNNLayer(in_features, self.hidden_dim, dropout)]\n",
        "        else:\n",
        "            layers = [GNNLayer(in_features, out_features, dropout)]\n",
        "        for _ in range(self.n_layers-2):\n",
        "            layers.append(GNNLayer(self.hidden_dim, self.hidden_dim, dropout))\n",
        "        if self.n_layers > 1:\n",
        "            layers.append(GNNLayer(self.hidden_dim, out_features, dropout))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.n_layers - 1):\n",
        "            x = self.layers[i](x)\n",
        "            x = nn.ReLU(x)\n",
        "        x = self.layers[-1](x)\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "        #x = 1.0 + x - x.max(dim=-1, keepdim=True).values\n",
        "        #x = torch.clamp(x, 0, 1)\n",
        "        #x = x / x.sum(dim=-1, keepdim=True) #normalize st sum = 1\n",
        "        return x"
      ],
      "metadata": {
        "id": "fwmyXMYvTcHf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibPTM41DTQ1c",
        "outputId": "7556e334-6ad5-4250-9d36-54ef31e9ad83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 Modularity: -0.00552791 time: 0.0021s\n",
            "Epoch: 1501 Modularity: 0.47081259 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.47875404 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.48033568 time: 0.0006s\n",
            "Epoch: 6001 Modularity: 0.48087782 time: 0.0006s\n",
            "Epoch: 7501 Modularity: 0.48110086 time: 0.0006s\n",
            "Epoch: 9001 Modularity: 0.48119995 time: 0.0007s\n",
            "Epoch: 10501 Modularity: 0.48124561 time: 0.0007s\n",
            "Epoch: 12001 Modularity: 0.48126698 time: 0.0007s\n",
            "Epoch: 13501 Modularity: 0.48127708 time: 0.0006s\n",
            "Epoch: 15001 Modularity: 0.48128188 time: 0.0007s\n",
            "Epoch: 16501 Modularity: 0.48128411 time: 0.0007s\n",
            "Epoch: 18001 Modularity: 0.48128518 time: 0.0006s\n",
            "Epoch: 19501 Modularity: 0.48128569 time: 0.0007s\n",
            "Epoch: 21001 Modularity: 0.48128593 time: 0.0006s\n",
            "Epoch: 22501 Modularity: 0.48128608 time: 0.0016s\n",
            "Epoch: 24001 Modularity: 0.48128614 time: 0.0006s\n",
            "Epoch: 25501 Modularity: 0.48128617 time: 0.0006s\n",
            "Epoch: 27001 Modularity: 0.48128617 time: 0.0006s\n",
            "Epoch: 28501 Modularity: 0.48128617 time: 0.0006s\n",
            "Epoch: 30000 Modularity: 0.48128620 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.6657s\n",
            "tensor(-0.4813, grad_fn=<NegBackward0>)\n",
            "Epoch: 0001 Modularity: -0.00213463 time: 0.0008s\n",
            "Epoch: 1501 Modularity: 0.49551776 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.50565022 time: 0.0006s\n",
            "Epoch: 4501 Modularity: 0.50733244 time: 0.0006s\n",
            "Epoch: 6001 Modularity: 0.50789893 time: 0.0006s\n",
            "Epoch: 7501 Modularity: 0.50813127 time: 0.0006s\n",
            "Epoch: 9001 Modularity: 0.50823456 time: 0.0007s\n",
            "Epoch: 10501 Modularity: 0.50828207 time: 0.0006s\n",
            "Epoch: 12001 Modularity: 0.50830436 time: 0.0006s\n",
            "Epoch: 13501 Modularity: 0.50831485 time: 0.0006s\n",
            "Epoch: 15001 Modularity: 0.50831980 time: 0.0007s\n",
            "Epoch: 16501 Modularity: 0.50832218 time: 0.0006s\n",
            "Epoch: 18001 Modularity: 0.50832331 time: 0.0009s\n",
            "Epoch: 19501 Modularity: 0.50832385 time: 0.0006s\n",
            "Epoch: 21001 Modularity: 0.50832409 time: 0.0007s\n",
            "Epoch: 22501 Modularity: 0.50832421 time: 0.0006s\n",
            "Epoch: 24001 Modularity: 0.50832427 time: 0.0006s\n",
            "Epoch: 25501 Modularity: 0.50832427 time: 0.0006s\n",
            "Epoch: 27001 Modularity: 0.50832433 time: 0.0007s\n",
            "Epoch: 28501 Modularity: 0.50832433 time: 0.0006s\n",
            "Epoch: 30000 Modularity: 0.50832433 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.6494s\n",
            "tensor(-0.5083, grad_fn=<NegBackward0>)\n",
            "Epoch: 0001 Modularity: -0.00301959 time: 0.0007s\n",
            "Epoch: 1501 Modularity: 0.49118534 time: 0.0007s\n",
            "Epoch: 3001 Modularity: 0.49904785 time: 0.0007s\n",
            "Epoch: 4501 Modularity: 0.50054049 time: 0.0006s\n",
            "Epoch: 6001 Modularity: 0.50104958 time: 0.0006s\n",
            "Epoch: 7501 Modularity: 0.50125897 time: 0.0006s\n",
            "Epoch: 9001 Modularity: 0.50135201 time: 0.0006s\n",
            "Epoch: 10501 Modularity: 0.50139487 time: 0.0006s\n",
            "Epoch: 12001 Modularity: 0.50141495 time: 0.0007s\n",
            "Epoch: 13501 Modularity: 0.50142443 time: 0.0007s\n",
            "Epoch: 15001 Modularity: 0.50142896 time: 0.0006s\n",
            "Epoch: 16501 Modularity: 0.50143105 time: 0.0006s\n",
            "Epoch: 18001 Modularity: 0.50143200 time: 0.0006s\n",
            "Epoch: 19501 Modularity: 0.50143248 time: 0.0006s\n",
            "Epoch: 21001 Modularity: 0.50143272 time: 0.0006s\n",
            "Epoch: 22501 Modularity: 0.50143284 time: 0.0007s\n",
            "Epoch: 24001 Modularity: 0.50143290 time: 0.0007s\n",
            "Epoch: 25501 Modularity: 0.50143290 time: 0.0006s\n",
            "Epoch: 27001 Modularity: 0.50143290 time: 0.0006s\n",
            "Epoch: 28501 Modularity: 0.50143290 time: 0.0007s\n",
            "Epoch: 30000 Modularity: 0.50143290 time: 0.0007s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.6905s\n",
            "tensor(-0.5014, grad_fn=<NegBackward0>)\n",
            "Epoch: 0001 Modularity: -0.01046793 time: 0.0007s\n",
            "Epoch: 1501 Modularity: 0.54049540 time: 0.0006s\n",
            "Epoch: 3001 Modularity: 0.55075365 time: 0.0007s\n",
            "Epoch: 4501 Modularity: 0.55430633 time: 0.0006s\n",
            "Epoch: 6001 Modularity: 0.55497384 time: 0.0009s\n",
            "Epoch: 7501 Modularity: 0.55524164 time: 0.0007s\n",
            "Epoch: 9001 Modularity: 0.55536026 time: 0.0006s\n",
            "Epoch: 10501 Modularity: 0.55541492 time: 0.0006s\n",
            "Epoch: 12001 Modularity: 0.55544037 time: 0.0006s\n",
            "Epoch: 13501 Modularity: 0.55545247 time: 0.0006s\n",
            "Epoch: 15001 Modularity: 0.55545813 time: 0.0010s\n",
            "Epoch: 16501 Modularity: 0.55546087 time: 0.0011s\n",
            "Epoch: 18001 Modularity: 0.55546206 time: 0.0007s\n",
            "Epoch: 19501 Modularity: 0.55546272 time: 0.0007s\n",
            "Epoch: 21001 Modularity: 0.55546302 time: 0.0007s\n",
            "Epoch: 22501 Modularity: 0.55546314 time: 0.0006s\n",
            "Epoch: 24001 Modularity: 0.55546319 time: 0.0006s\n",
            "Epoch: 25501 Modularity: 0.55546325 time: 0.0006s\n",
            "Epoch: 27001 Modularity: 0.55546325 time: 0.0007s\n",
            "Epoch: 28501 Modularity: 0.55546325 time: 0.0006s\n",
            "Epoch: 30000 Modularity: 0.55546325 time: 0.0007s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.6170s\n",
            "tensor(-0.5555, grad_fn=<NegBackward0>)\n",
            "Epoch: 0001 Modularity: 0.00048873 time: 0.0008s\n",
            "Epoch: 1501 Modularity: 0.53917134 time: 0.0011s\n",
            "Epoch: 3001 Modularity: 0.54901552 time: 0.0007s\n",
            "Epoch: 4501 Modularity: 0.55169201 time: 0.0006s\n",
            "Epoch: 6001 Modularity: 0.55233711 time: 0.0007s\n",
            "Epoch: 7501 Modularity: 0.55260074 time: 0.0007s\n",
            "Epoch: 9001 Modularity: 0.55271786 time: 0.0006s\n",
            "Epoch: 10501 Modularity: 0.55277181 time: 0.0007s\n",
            "Epoch: 12001 Modularity: 0.55279714 time: 0.0006s\n",
            "Epoch: 13501 Modularity: 0.55280900 time: 0.0006s\n",
            "Epoch: 15001 Modularity: 0.55281466 time: 0.0006s\n",
            "Epoch: 16501 Modularity: 0.55281734 time: 0.0006s\n",
            "Epoch: 18001 Modularity: 0.55281860 time: 0.0006s\n",
            "Epoch: 19501 Modularity: 0.55281919 time: 0.0006s\n",
            "Epoch: 21001 Modularity: 0.55281949 time: 0.0007s\n",
            "Epoch: 22501 Modularity: 0.55281961 time: 0.0007s\n",
            "Epoch: 24001 Modularity: 0.55281967 time: 0.0011s\n",
            "Epoch: 25501 Modularity: 0.55281973 time: 0.0006s\n",
            "Epoch: 27001 Modularity: 0.55281973 time: 0.0006s\n",
            "Epoch: 28501 Modularity: 0.55281973 time: 0.0007s\n",
            "Epoch: 30000 Modularity: 0.55281973 time: 0.0006s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 19.6648s\n",
            "tensor(-0.5528, grad_fn=<NegBackward0>)\n",
            "-0.4812861979007721\n"
          ]
        }
      ],
      "source": [
        "features = torch.FloatTensor(X.transpose())\n",
        "best_best_mod = -1\n",
        "for seed in range(5):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    t_total = time.time()\n",
        "    n_comm = max(combo_comms.values()) + 2\n",
        "    model = GNN_MLP(features.shape[1], n_comm)\n",
        "    lr = 0.002\n",
        "    n_epochs = 30000\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(n_epochs):\n",
        "        t_1run = time.time()\n",
        "        optimizer.zero_grad()\n",
        "        out_embed = model(features)\n",
        "        C = out_embed#[:, :n_comm]\n",
        "        Q1 = torch.mm(C.T, Q)\n",
        "        Q2 = torch.mm(Q1, C)\n",
        "        loss = torch.trace(Q2)\n",
        "        loss = -loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch == 0 or loss < best_loss:\n",
        "            best_loss = loss #- torch.trace(Q)\n",
        "            best_C = C.data\n",
        "            best_embed = out_embed.data\n",
        "            best_epoch = epoch\n",
        "        if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
        "            #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            print('Epoch: {:04d}'.format(epoch + 1),\n",
        "                    'Modularity: {:.8f}'.format(-best_loss.item()),\n",
        "                    'time: {:.4f}s'.format(time.time() - t_1run))\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "    print(best_loss)\n",
        "    best_best_mod = max(best_best_mod, -best_loss.item())\n",
        "    #print(best_embed)\n",
        "print(best_best_mod)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Whnxlk_EZ0Iy"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}