{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L73c8k8DtR2w",
        "outputId": "a885df42-5c19-4c17-cb08-e687a95a0cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycombo\n",
            "  Downloading pycombo-0.1.7.tar.gz (136 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 136 kB 4.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11<3.0.0,>=2.6.1\n",
            "  Downloading pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     |████████████████████████████████| 213 kB 33.9 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata<2.0,>=1.0\n",
            "  Using cached importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<2.0,>=1.0->pycombo) (3.8.0)\n",
            "Building wheels for collected packages: pycombo\n",
            "  Building wheel for pycombo (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycombo: filename=pycombo-0.1.7-cp37-cp37m-manylinux_2_27_x86_64.whl size=98830 sha256=9e19beca46226f3c9742a00b868b377976ed462204d0dc244d4b1c47ed62f386\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/52/18/4c1b80cd45c091e2c1ea442729343ac984dc66b3a678e2c251\n",
            "Successfully built pycombo\n",
            "Installing collected packages: pybind11, importlib-metadata, pycombo\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.3\n",
            "    Uninstalling importlib-metadata-4.11.3:\n",
            "      Successfully uninstalled importlib-metadata-4.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed importlib-metadata-1.7.0 pybind11-2.9.2 pycombo-0.1.7\n",
            "Collecting fastnode2vec\n",
            "  Downloading fastnode2vec-0.0.6-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (7.1.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (1.21.6)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from fastnode2vec) (0.51.2)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->fastnode2vec) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->fastnode2vec) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->fastnode2vec) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->fastnode2vec) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->fastnode2vec) (0.34.0)\n",
            "Installing collected packages: fastnode2vec\n",
            "Successfully installed fastnode2vec-0.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pycombo\n",
        "!pip install fastnode2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aH23YvKuTW9y"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pycombo\n",
        "import fastnode2vec\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.set_printoptions(sci_mode=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "S7bPMdVUTkPm"
      },
      "outputs": [],
      "source": [
        "def modularity_matrix(adj):\n",
        "    w_in = adj.sum(dim=0, keepdim=True)\n",
        "    w_out = adj.sum(dim=1, keepdim=True)\n",
        "    T = w_out.sum()\n",
        "    Q = adj / T - w_out * w_in / T ** 2\n",
        "    return Q\n",
        "\n",
        "def modularity(Q, partition):\n",
        "    return (Q * (partition.reshape(-1,1) == partition.reshape(1,-1))).sum()\n",
        "\n",
        "def residential_page_rank_embedding(A, alpha=0.85):\n",
        "    A = A / A.sum(axis = 1)\n",
        "    n = A.shape[0]\n",
        "    AI = np.linalg.inv(np.eye(n) - A * alpha)\n",
        "    X = (1 - alpha) * AI\n",
        "    return X.transpose()\n",
        "\n",
        "def node2vec_embedding(G: nx.Graph, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42):\n",
        "    if nx.is_weighted(G):\n",
        "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1]), edge[2]['weight']) for edge in G.edges(data=True)],\n",
        "                directed=False, weighted=True)\n",
        "    else:\n",
        "        n2v_graph = fastnode2vec.Graph([(str(edge[0]), str(edge[1])) for edge in G.edges(data=True)],\n",
        "                    directed=False, weighted=False)\n",
        "    n2v = fastnode2vec.Node2Vec(n2v_graph, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
        "    n2v.train(epochs=100)\n",
        "    n2v_embeddings = np.array([n2v.wv[str(node)] for node in G])\n",
        "    return n2v_embeddings\n",
        "\n",
        "def rpr_clustering(A: np.array, n_clusters=4, kmeans_runs=100, alpha=0.85, seed=42):\n",
        "    rpr_embedding = residential_page_rank_embedding(A, alpha)\n",
        "    scaled_embedding = StandardScaler().fit_transform(X=rpr_embedding)\n",
        "    rpr_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(scaled_embedding).labels_\n",
        "    return rpr_cluster_labels\n",
        "\n",
        "def n2v_clustering(G: nx.Graph, n_clusters=4, kmeans_runs=100, dim=10, walk_length=100, context=10, p=2.0, q=0.5, workers=2, seed=42):\n",
        "    n2v_embeddings = node2vec_embedding(G, dim=dim, walk_length=walk_length, context=context, p=p, q=q, workers=workers, seed=seed)\n",
        "    scaled_embedding = StandardScaler().fit_transform(X=n2v_embeddings)\n",
        "    n2v_cluster_labels = KMeans(n_clusters=n_clusters, n_init=kmeans_runs, random_state=seed).fit(scaled_embedding).labels_\n",
        "    return n2v_cluster_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "Gs =  [nx.from_numpy_array(nx.to_numpy_array(nx.karate_club_graph(), weight=None)), nx.les_miserables_graph()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPDPOtUuTnqH",
        "outputId": "2f0d1fb3-cb87-4585-d64e-7d73ed6b4e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading graph: 100%|██████████| 78/78 [00:00<00:00, 341855.50it/s]\n",
            "Training: 100%|██████████| 3400/3400 [00:00<00:00, 9880.26it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combo: 0.41978961209730403 , resid. page rank k-means: 0.4197896122932434 , node2vec kmeans: 0.39340895414352417\n",
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading graph: 100%|██████████| 254/254 [00:00<00:00, 495282.76it/s]\n",
            "Training: 100%|██████████| 7700/7700 [00:01<00:00, 5833.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combo: 0.566687983343249 , resid. page rank k-means: 0.5635915994644165 , node2vec kmeans: 0.5367683172225952\n",
            "------------------------------------\n",
            "SEED =  1\n",
            "Processing graph... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading graph: 100%|██████████| 78/78 [00:00<00:00, 191431.08it/s]\n",
            "Training: 100%|██████████| 3400/3400 [00:00<00:00, 10368.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combo: 0.41978961209730403 , resid. page rank k-means: 0.411160409450531 , node2vec kmeans: 0.411160409450531\n",
            "------------------------------------\n",
            "SEED =  1\n",
            "Processing graph... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading graph: 100%|██████████| 254/254 [00:00<00:00, 389241.22it/s]\n",
            "Training: 100%|██████████| 7700/7700 [00:01<00:00, 6057.24it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combo: 0.566687983343249 , resid. page rank k-means: 0.5635915994644165 , node2vec kmeans: 0.5451078414916992\n",
            "------------------------------------\n",
            "SEED =  2\n",
            "Processing graph... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading graph: 100%|██████████| 78/78 [00:00<00:00, 306900.29it/s]\n",
            "Training: 100%|██████████| 3400/3400 [00:00<00:00, 10060.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combo: 0.41978961209730403 , resid. page rank k-means: 0.4197896122932434 , node2vec kmeans: 0.4106673300266266\n",
            "------------------------------------\n",
            "SEED =  2\n",
            "Processing graph... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading graph: 100%|██████████| 254/254 [00:00<00:00, 322834.31it/s]\n",
            "Training: 100%|██████████| 7700/7700 [00:01<00:00, 5942.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combo: 0.566687983343249 , resid. page rank k-means: 0.5654157400131226 , node2vec kmeans: 0.5382205247879028\n"
          ]
        }
      ],
      "source": [
        "for SEED in range(3):\n",
        "    for G in Gs:\n",
        "        print(\"------------------------------------\")\n",
        "        print(\"SEED = \", SEED)\n",
        "        print(\"Processing graph...\", G.name)\n",
        "        combo_comms, combo_mod = pycombo.execute(G)\n",
        "        n_comms = np.unique(list(combo_comms.values())).size\n",
        "        A = nx.to_numpy_array(G)\n",
        "        adj = torch.FloatTensor(A)\n",
        "        Q = modularity_matrix(adj)\n",
        "        rpr_C = rpr_clustering(A, n_clusters=n_comms, seed=SEED)\n",
        "        n2v_C = n2v_clustering(G, n_clusters=n_comms, seed=SEED)\n",
        "        print('combo:', combo_mod, ', resid. page rank k-means:', modularity(Q, rpr_C).item(), ', node2vec kmeans:', modularity(Q, n2v_C).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fwmyXMYvTcHf"
      },
      "outputs": [],
      "source": [
        "class GNNLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(GNNLayer, self).__init__()\n",
        "        self.weight1 = nn.Parameter(torch.randn(in_features, out_features)) # 0.5 * torch.eye(in_features, out_features))\n",
        "        self.bias = nn.Parameter(torch.randn(1, out_features)) # -0.5 * torch.ones(1, out_features))\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, input):\n",
        "        v1 = torch.mm(input, self.weight1)\n",
        "        output = v1 + self.bias\n",
        "        output = F.dropout(output, p=self.dropout, training=self.training)\n",
        "        return output\n",
        "\n",
        "class GNN_MLP(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_layers=1, hidden_dim=8, dropout=0.0):\n",
        "        super(GNN_MLP, self).__init__()\n",
        "        self.n_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        if self.n_layers > 1:\n",
        "            layers = [GNNLayer(in_features, self.hidden_dim, dropout)]\n",
        "        else:\n",
        "            layers = [GNNLayer(in_features, out_features, dropout)]\n",
        "        for _ in range(self.n_layers-2):\n",
        "            layers.append(GNNLayer(self.hidden_dim, self.hidden_dim, dropout))\n",
        "        if self.n_layers > 1:\n",
        "            layers.append(GNNLayer(self.hidden_dim, out_features, dropout))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.n_layers - 1):\n",
        "            x = self.layers[i](x)\n",
        "            x = nn.ReLU()(x)\n",
        "        x = self.layers[-1](x)\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "        #x = 1.0 + x - x.max(dim=-1, keepdim=True).values\n",
        "        #x = torch.clamp(x, 0, 1)\n",
        "        #x = x / x.sum(dim=-1, keepdim=True) #normalize st sum = 1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibPTM41DTQ1c",
        "outputId": "6a235bb3-ddef-4259-f3b5-ed9ff7b433c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: 0.00106575 time: 0.1562s\n",
            "Epoch: 0301 Modularity: 0.08697913 time: 0.0005s\n",
            "Epoch: 0601 Modularity: 0.21221571 time: 0.0359s\n",
            "Epoch: 0901 Modularity: 0.27102804 time: 0.0005s\n",
            "Epoch: 1201 Modularity: 0.30237350 time: 0.0005s\n",
            "Epoch: 1501 Modularity: 0.32112288 time: 0.0006s\n",
            "Epoch: 1801 Modularity: 0.33329281 time: 0.0850s\n",
            "Epoch: 2101 Modularity: 0.34167421 time: 0.0107s\n",
            "Epoch: 2401 Modularity: 0.34770837 time: 0.0007s\n",
            "Epoch: 2701 Modularity: 0.35220337 time: 0.0007s\n",
            "Epoch: 3001 Modularity: 0.35564300 time: 0.0007s\n",
            "Epoch: 3301 Modularity: 0.35833240 time: 0.0157s\n",
            "Epoch: 3601 Modularity: 0.36047223 time: 0.0079s\n",
            "Epoch: 3901 Modularity: 0.36219963 time: 0.0015s\n",
            "Epoch: 4201 Modularity: 0.36361083 time: 0.0005s\n",
            "Epoch: 4501 Modularity: 0.36477530 time: 0.0005s\n",
            "Epoch: 4801 Modularity: 0.36574432 time: 0.0842s\n",
            "Epoch: 5101 Modularity: 0.36655658 time: 0.1060s\n",
            "Epoch: 5401 Modularity: 0.36724141 time: 0.0028s\n",
            "Epoch: 5701 Modularity: 0.36782193 time: 0.0008s\n",
            "Epoch: 6000 Modularity: 0.36831468 time: 0.0010s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 87.2203s\n",
            "tensor(-0.3683, grad_fn=<NegBackward0>)\n",
            "------------------------------------\n",
            "SEED =  0\n",
            "Processing graph... \n",
            "Epoch: 0001 Modularity: 0.00246835 time: 0.0961s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/alex/Dropbox/projects/python/GNNs/Community-Detection/Notebook/Embedding_NN_Clustering.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Dropbox/projects/python/GNNs/Community-Detection/Notebook/Embedding_NN_Clustering.ipynb#ch0000006?line=24'>25</a>\u001b[0m out_embed \u001b[39m=\u001b[39m model(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Dropbox/projects/python/GNNs/Community-Detection/Notebook/Embedding_NN_Clustering.ipynb#ch0000006?line=25'>26</a>\u001b[0m C \u001b[39m=\u001b[39m out_embed\u001b[39m#[:, :n_comm]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alex/Dropbox/projects/python/GNNs/Community-Detection/Notebook/Embedding_NN_Clustering.ipynb#ch0000006?line=26'>27</a>\u001b[0m Q1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmm(C\u001b[39m.\u001b[39;49mT, Q)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Dropbox/projects/python/GNNs/Community-Detection/Notebook/Embedding_NN_Clustering.ipynb#ch0000006?line=27'>28</a>\u001b[0m Q2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(Q1, C)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alex/Dropbox/projects/python/GNNs/Community-Detection/Notebook/Embedding_NN_Clustering.ipynb#ch0000006?line=28'>29</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtrace(Q2)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for SEED in range(3):\n",
        "    for G in Gs:\n",
        "        print(\"------------------------------------\")\n",
        "        print(\"SEED = \", SEED)\n",
        "        print(\"Processing graph...\", G.name)\n",
        "        combo_comms, combo_mod = pycombo.execute(G)\n",
        "        n_comms = np.unique(list(combo_comms.values())).size\n",
        "        A = nx.to_numpy_array(G)\n",
        "        adj = torch.FloatTensor(A)\n",
        "        Q = modularity_matrix(adj)\n",
        "        \n",
        "        rpr_embedding = residential_page_rank_embedding(A, 0.85)\n",
        "        features = torch.FloatTensor(rpr_embedding)\n",
        "        best_best_mod = -1\n",
        "        np.random.seed(SEED)\n",
        "        torch.manual_seed(SEED)\n",
        "        t_total = time.time()\n",
        "        model = GNN_MLP(features.shape[1], n_comms + 2)\n",
        "        lr = 0.002\n",
        "        n_epochs = 6000\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        for epoch in range(n_epochs):\n",
        "            t_1run = time.time()\n",
        "            optimizer.zero_grad()\n",
        "            out_embed = model(features)\n",
        "            C = out_embed#[:, :n_comm]\n",
        "            Q1 = torch.mm(C.T, Q)\n",
        "            Q2 = torch.mm(Q1, C)\n",
        "            loss = torch.trace(Q2)\n",
        "            loss = -loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if epoch == 0 or loss < best_loss:\n",
        "                best_loss = loss #- torch.trace(Q)\n",
        "                best_C = C.data\n",
        "                best_embed = out_embed.data\n",
        "                best_epoch = epoch\n",
        "            if n_epochs <= 20 or epoch % (n_epochs//20) == 0 or epoch == n_epochs - 1:\n",
        "                #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                print('Epoch: {:04d}'.format(epoch + 1),\n",
        "                        'Modularity: {:.8f}'.format(-best_loss.item()),\n",
        "                        'time: {:.4f}s'.format(time.time() - t_1run))\n",
        "        print(\"Optimization Finished!\")\n",
        "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "        print(best_loss)\n",
        "        best_best_mod = max(best_best_mod, -best_loss.item())\n",
        "        #print(best_embed)\n",
        "    print(best_best_mod)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Whnxlk_EZ0Iy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Embedding_NN_Clustering.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "aa8c9e988bba47ec3e791b22c0bbb49eb66a938b6cde8fffd564472f09fd563a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
